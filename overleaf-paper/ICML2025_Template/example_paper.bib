@article{achiam2023gpt,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{liu2023visual,
  title={Visual Instruction Tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2023}
}

@article{li2023blip,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and others},
  journal={ICML},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and others},
  journal={NeurIPS},
  year={2022}
}

@article{zhu2023minigpt,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and others},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{dai2023instructblip,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Dai, Wenliang and others},
  journal={NeurIPS},
  year={2023}
}

@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and others},
  journal={ICCV},
  year={2023}
}

@article{zou2023seem,
  title={Segment Everything Everywhere All at Once},
  author={Zou, Xueyan and others},
  journal={NeurIPS},
  year={2023}
}

@article{lai2023lisa,
  title={LISA: Reasoning Segmentation via Large Language Model},
  author={Lai, Xin and others},
  journal={arXiv preprint arXiv:2308.00692},
  year={2023}
}

@article{ke2023segment,
  title={Segment Anything in High Quality},
  author={Ke, Lei and others},
  journal={NeurIPS},
  year={2023}
}

@article{zhang2023personalize,
  title={Personalize Segment Anything Model with One Shot},
  author={Zhang, Renrui and others},
  journal={arXiv preprint arXiv:2305.03048},
  year={2023}
}

@article{li2023semantic,
  title={Semantic-SAM: Segment and Recognize Anything at Any Granularity},
  author={Li, Feng and others},
  journal={arXiv preprint arXiv:2307.04767},
  year={2023}
}

@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and others},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@article{ren2023pixellm,
  title={PixelLM: Pixel Reasoning with Large Multimodal Models},
  author={Ren, Zhongwei and others},
  journal={arXiv preprint arXiv:2312.02228},
  year={2023}
}

@article{rasheed2024glamm,
  title={GLaMM: Pixel Grounding Large Multimodal Model},
  author={Rasheed, Hanoona and others},
  journal={CVPR},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen-VL: A Frontier of Vision-Language Model},
  author={Bai, Jinze and others},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{young2024yi,
  title={Yi: Open Foundation Models by 01.AI},
  author={Young, Alex and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{lu2024deepseek,
  title={DeepSeek-VL: Towards Real-World Vision-Language Understanding},
  author={Lu, Haoyu and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{huang2023language,
  title={Language Is Not All You Need: Aligning Perception with Language Models},
  author={Huang, Shaohan and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}

@article{peng2023kosmos,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Peng, Zhiliang and others},
  journal={arXiv preprint arXiv:2306.14824},
  year={2023}
}

@article{driess2023palm,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Driess, Danny and others},
  journal={ICML},
  year={2023}
}

@article{wang2023cogvlm,
  title={CogVLM: Visual Expert for Pretrained Language Models},
  author={Wang, Weihan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and others},
  journal={CVPR},
  year={2024}
}

@article{liu2023improved,
  title={Improved Baselines with Visual Instruction Tuning},
  author={Liu, Haotian and others},
  journal={CVPR},
  year={2024}
}

@article{liu2023simpleclick,
  title={SimpleClick: Interactive Image Segmentation with Simple Vision Transformers},
  author={Liu, Qin and others},
  journal={ICCV},
  year={2023}
}

@article{chen2022focalclick,
  title={FocalClick: Towards Practical Interactive Image Segmentation},
  author={Chen, Xi and others},
  journal={CVPR},
  year={2022}
}

@article{sofiiuk2022reviving,
  title={Reviving Iterative Training with Mask Guidance for Interactive Segmentation},
  author={Sofiiuk, Konstantin and others},
  journal={ICIP},
  year={2022}
}

@article{shi2024mdvp,
  title={MDVP-Bench: A Benchmark for Multimodal Fine-grained Visual Prompting},
  author={Shi, Mingqi and others},
  journal={arXiv preprint arXiv:2403.20271},
  year={2024}
}

@article{bahng2022visual,
  title={Visual Prompting},
  author={Bahng, Hyojin and others},
  journal={arXiv preprint arXiv:2203.17274},
  year={2022}
}

@article{wang2023images,
  title={Images Speak in Images: A Generalist Painter for In-Context Visual Learning},
  author={Wang, Xinlong and others},
  journal={CVPR},
  year={2023}
}

@article{wang2023seggpt,
  title={SegGPT: Segmenting Everything In Context},
  author={Wang, Xinlong and others},
  journal={ICCV},
  year={2023}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and others},
  journal={ICML},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and others},
  journal={ICLR},
  year={2021}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
